import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import os

# --- é…ç½® ---
# è¨­å®š Ollama LLM æ¨¡å‹åç¨±
LLM_MODEL = "deepseek-coder:v2.3-lite"

# è¨­å®šåµŒå…¥æ¨¡å‹
EMBEDDING_MODEL = "deepseek-coder:v2.3-lite" # Ollama ä¹Ÿå¯ä»¥ç”¨æ–¼åµŒå…¥

# --- Streamlit ä»‹é¢ ---
st.set_page_config(page_title="PDF RAG èŠå¤©æ©Ÿå™¨äºº", layout="wide")
st.title("ğŸ“„ PDF RAG èŠå¤©æ©Ÿå™¨äºº (Parent-Child Strategy)")
st.markdown("""
    ä½¿ç”¨ **Modular RAG** æŠ€è¡“å’Œ **Parent-Child** åˆ†å¡Šç­–ç•¥ï¼Œ
    è®“ä½ å¯ä»¥ä¸Šå‚³ PDF æª”æ¡ˆä¸¦ä½¿ç”¨é›¢ç·šçš„ `deepseek-coder:v2.3-lite` LLM é€²è¡Œå•ç­”ã€‚
""")

# --- åˆå§‹åŒ– Session ç‹€æ…‹ ---
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "llm_chain" not in st.session_state:
    st.session_state.llm_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- é¡¯ç¤ºæ­·å²è¨Šæ¯ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- å´é‚Šæ¬„ä¸Šå‚³ PDF ---
with st.sidebar:
    st.header("ä¸Šå‚³ä½ çš„ PDF æª”æ¡ˆ")
    uploaded_file = st.file_uploader("é¸æ“‡ä¸€å€‹ PDF æª”æ¡ˆ", type="pdf")

    if uploaded_file:
        st.success(f"æª”æ¡ˆ '{uploaded_file.name}' å·²ä¸Šå‚³ï¼")
        if st.button("è™•ç† PDF"):
            with st.spinner("æ­£åœ¨è™•ç† PDF æª”æ¡ˆä¸¦å»ºç«‹çŸ¥è­˜åº«...é€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ã€‚"):
                # 1. è¼‰å…¥ PDF
                temp_file_path = f"./temp_{uploaded_file.name}"
                with open(temp_file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                loader = PyPDFLoader(temp_file_path)
                docs = loader.load()

                # 2. å®šç¾©æ–‡æœ¬åˆ†å¡Šå™¨ (Parent-Child Strategy)
                # å­å€å¡Šåˆ†å¡Šå™¨ (ç”¨æ–¼åµŒå…¥å’Œæª¢ç´¢)
                child_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=400, # å­å€å¡Šå¤§å°
                    chunk_overlap=50,
                    length_function=len,
                    is_separator_regex=False,
                )

                # çˆ¶å€å¡Šåˆ†å¡Šå™¨ (ç”¨æ–¼å‚³éçµ¦ LLM)
                parent_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=2000, # çˆ¶å€å¡Šå¤§å°
                    chunk_overlap=200,
                    length_function=len,
                    is_separator_regex=False,
                )

                # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
                embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

                # 4. åˆå§‹åŒ–å‘é‡å„²å­˜å’Œè¨˜æ†¶é«”å„²å­˜
                vectorstore = FAISS.from_documents(docs, embeddings) # é€™è£¡å…ˆç”¨æ‰€æœ‰æ–‡ä»¶åˆå§‹åŒ–ï¼ŒParentDocumentRetriever æœƒè™•ç†å­å€å¡Š
                
                # è¨˜æ†¶é«”å„²å­˜ç”¨æ–¼å„²å­˜çˆ¶å€å¡Š
                store = InMemoryStore()

                # 5. å»ºç«‹ ParentDocumentRetriever
                # é€™å€‹ retriever æœƒå°‡å­å€å¡Šå„²å­˜åœ¨ vectorstore ä¸­ï¼Œä¸¦å°‡çˆ¶å€å¡Šå„²å­˜åœ¨ store ä¸­
                retriever = ParentDocumentRetriever(
                    vectorstore=vectorstore,
                    docstore=store,
                    child_splitter=child_splitter,
                    parent_splitter=parent_splitter,
                )
                # å°‡åŸå§‹æ–‡ä»¶æ·»åŠ åˆ° retriever ä¸­ï¼Œé€™æœƒè‡ªå‹•é€²è¡Œçˆ¶å­åˆ†å¡Šä¸¦å„²å­˜
                retriever.add_documents(docs)

                st.session_state.retriever = retriever

                # 6. åˆå§‹åŒ– LLM
                llm = Ollama(model=LLM_MODEL)

                # 7. å»ºç«‹ RAG éˆ
                # å®šç¾©ç”¨æ–¼ç”Ÿæˆç­”æ¡ˆçš„æç¤ºæ¨¡æ¿
                prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚
                å¦‚æœå•é¡Œç„¡æ³•å¾æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè«‹èª å¯¦åœ°èªªä½ ä¸çŸ¥é“ã€‚
                è«‹ä¿æŒç­”æ¡ˆç°¡æ½”æ˜ç­ã€‚

                ä¸Šä¸‹æ–‡:
                {context}

                å•é¡Œ: {input}
                """)

                # å»ºç«‹æ–‡ä»¶åˆä½µéˆï¼Œå°‡æª¢ç´¢åˆ°çš„æ–‡ä»¶çµ„åˆæˆ LLM çš„è¼¸å…¥
                document_chain = create_stuff_documents_chain(llm, prompt)

                # å»ºç«‹æª¢ç´¢éˆï¼Œå®ƒå°‡æª¢ç´¢ç›¸é—œæ–‡æª”ä¸¦å°‡å…¶å‚³éçµ¦ document_chain
                retrieval_chain = create_retrieval_chain(st.session_state.retriever, document_chain)
                
                st.session_state.llm_chain = retrieval_chain

                # ç§»é™¤è‡¨æ™‚æ–‡ä»¶
                os.remove(temp_file_path)
                
                st.success("PDF è™•ç†å®Œæˆï¼Œæ‚¨å¯ä»¥é–‹å§‹æå•äº†ï¼")
                st.info("è«‹æ³¨æ„ï¼šç¬¬ä¸€æ¬¡é‹è¡Œ Ollama æ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ä¾†è¼‰å…¥ã€‚")
    else:
        st.session_state.retriever = None
        st.session_state.llm_chain = None
        st.session_state.messages = []
        st.warning("è«‹ä¸Šå‚³ä¸€å€‹ PDF æª”æ¡ˆä¾†é–‹å§‹ã€‚")

# --- èŠå¤©è¼¸å…¥æ¡† ---
if st.session_state.llm_chain:
    if prompt_input := st.chat_input("è«‹è¼¸å…¥ä½ çš„å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": prompt_input})
        with st.chat_message("user"):
            st.markdown(prompt_input)

        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                response = st.session_state.llm_chain.invoke({"input": prompt_input})
                assistant_response = response["answer"]
                st.markdown(assistant_response)
                st.session_state.messages.append({"role": "assistant", "content": assistant_response})
else:
    st.info("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³ä¸¦è™•ç†ä½ çš„ PDF æª”æ¡ˆã€‚")